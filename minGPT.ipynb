{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5266d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c56fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = 'text-sample.txt'\n",
    "with open(text_file, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d832cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504850"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c25c8715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4 [Ope23], was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT- 4 is part of a new cohort of LLMs (along with ChatGPT and Google’s PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4’s performa'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0411c891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}°±·×Øøˆ̧́̈ελπρσω–—‘’“”•′⃗→⇒∂∆∇∈−∗√∞∪≈≤≥⊕◦\\uf8ee\\uf8ef\\uf8f0\\uf8f9\\uf8fa\\uf8fb'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64ee4fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for i, c in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda int_list: ''.join([itos[i] for i in int_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f36ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 77, 66, 1, 78, 66, 1, 76, 80, 85, 66]\n",
      "Ala ma kota\n"
     ]
    }
   ],
   "source": [
    "ala_encoded = encode('Ala ma kota')\n",
    "ala_decoded = decode(ala_encoded)\n",
    "print(ala_encoded)\n",
    "print(ala_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a71458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c1af4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([504850])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a8790c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 34,  83,  85,  74,  71,  74,  68,  74,  66,  77,   1,  74,  79,  85,\n",
      "         70,  77,  77,  74,  72,  70,  79,  68,  70,   1,   9,  34,  42,  10,\n",
      "          1,  83,  70,  84,  70,  66,  83,  68,  73,  70,  83,  84,   1,  73,\n",
      "         66,  87,  70,   1,  67,  70,  70,  79,   1,  69,  70,  87,  70,  77,\n",
      "         80,  81,  74,  79,  72,   1,  66,  79,  69,   1,  83,  70,  71,  74,\n",
      "         79,  74,  79,  72,   1,  77,  66,  83,  72,  70,   1,  77,  66,  79,\n",
      "         72,  86,  66,  72,  70,   1,  78,  80,  69,  70,  77,  84,   1,   9,\n",
      "         45,  45,  46,  84,  10,   1,  85,  73,  66,  85,   1,  70,  89,  73,\n",
      "         74,  67,  74,  85,   1,  83,  70,  78,  66,  83,  76,  66,  67,  77,\n",
      "         70,   1,  68,  66,  81,  66,  67,  74,  77,  74,  85,  74,  70,  84,\n",
      "          1,  66,  68,  83,  80,  84,  84,   1,  66,   1,  87,  66,  83,  74,\n",
      "         70,  85,  90,   1,  80,  71,   1,  69,  80,  78,  66,  74,  79,  84,\n",
      "          1,  66,  79,  69,   1,  85,  66,  84,  76,  84,  13,   1,  68,  73,\n",
      "         66,  77,  77,  70,  79,  72,  74,  79,  72,   1,  80,  86,  83,   1,\n",
      "         86,  79,  69,  70,  83,  84,  85,  66,  79,  69,  74,  79,  72,   1,\n",
      "         80,  71,   1,  77,  70,  66,  83,  79,  74,  79,  72,   1,  66,  79,\n",
      "         69,   1,  68,  80,  72,  79,  74,  85,  74,  80,  79,  15,   1,  53,\n",
      "         73,  70,   1,  77,  66,  85,  70,  84,  85,   1,  78,  80,  69,  70,\n",
      "         77,   1,  69,  70,  87,  70,  77,  80,  81,  70,  69,   1,  67,  90,\n",
      "          1,  48,  81,  70,  79,  34,  42,  13,   1,  40,  49,  53,  14,  21,\n",
      "          1,  60,  48,  81,  70,  19,  20,  62,  13,   1,  88,  66,  84,   1,\n",
      "         85,  83,  66,  74,  79,  70,  69,   1,  86,  84,  74,  79,  72,   1,\n",
      "         66,  79,   1,  86,  79,  81,  83,  70,  68,  70,  69,  70,  79,  85,\n",
      "         70,  69,   1,  84,  68,  66,  77,  70,   1,  80,  71,   1,  68,  80,\n",
      "         78,  81,  86,  85,  70,   1,  66,  79,  69,   1,  69,  66,  85,  66,\n",
      "         15,   1,  42,  79,   1,  85,  73,  74,  84,   1,  81,  66,  81,  70,\n",
      "         83,  13,   1,  88,  70,   1,  83,  70,  81,  80,  83,  85,   1,  80,\n",
      "         79,   1,  80,  86,  83,   1,  74,  79,  87,  70,  84,  85,  74,  72,\n",
      "         66,  85,  74,  80,  79,   1,  80,  71,   1,  66,  79,   1,  70,  66,\n",
      "         83,  77,  90,   1,  87,  70,  83,  84,  74,  80,  79,   1,  80,  71,\n",
      "          1,  40,  49,  53,  14,  21,  13,   1,  88,  73,  70,  79,   1,  74,\n",
      "         85,   1,  88,  66,  84,   1,  84,  85,  74,  77,  77,   1,  74,  79,\n",
      "          1,  66,  68,  85,  74,  87,  70,   1,  69,  70,  87,  70,  77,  80,\n",
      "         81,  78,  70,  79,  85,   1,  67,  90,   1,  48,  81,  70,  79,  34,\n",
      "         42,  15,   1,  56,  70,   1,  68,  80,  79,  85,  70,  79,  69,   1,\n",
      "         85,  73,  66,  85,   1,   9,  85,  73,  74,  84,   1,  70,  66,  83,\n",
      "         77,  90,   1,  87,  70,  83,  84,  74,  80,  79,   1,  80,  71,  10,\n",
      "          1,  40,  49,  53,  14,   1,  21,   1,  74,  84,   1,  81,  66,  83,\n",
      "         85,   1,  80,  71,   1,  66,   1,  79,  70,  88,   1,  68,  80,  73,\n",
      "         80,  83,  85,   1,  80,  71,   1,  45,  45,  46,  84,   1,   9,  66,\n",
      "         77,  80,  79,  72,   1,  88,  74,  85,  73,   1,  36,  73,  66,  85,\n",
      "         40,  49,  53,   1,  66,  79,  69,   1,  40,  80,  80,  72,  77,  70,\n",
      "        114,  84,   1,  49,  66,  45,  46,   1,  71,  80,  83,   1,  70,  89,\n",
      "         66,  78,  81,  77,  70,  10,   1,  85,  73,  66,  85,   1,  70,  89,\n",
      "         73,  74,  67,  74,  85,   1,  78,  80,  83,  70,   1,  72,  70,  79,\n",
      "         70,  83,  66,  77,   1,  74,  79,  85,  70,  77,  77,  74,  72,  70,\n",
      "         79,  68,  70,   1,  85,  73,  66,  79,   1,  81,  83,  70,  87,  74,\n",
      "         80,  86,  84,   1,  34,  42,   1,  78,  80,  69,  70,  77,  84,  15,\n",
      "          1,  56,  70,   1,  69,  74,  84,  68,  86,  84,  84,   1,  85,  73,\n",
      "         70,   1,  83,  74,  84,  74,  79,  72,   1,  68,  66,  81,  66,  67,\n",
      "         74,  77,  74,  85,  74,  70,  84,   1,  66,  79,  69,   1,  74,  78,\n",
      "         81,  77,  74,  68,  66,  85,  74,  80,  79,  84,   1,  80,  71,   1,\n",
      "         85,  73,  70,  84,  70,   1,  78,  80,  69,  70,  77,  84,  15,   1,\n",
      "         56,  70,   1,  69,  70,  78,  80,  79,  84,  85,  83,  66,  85,  70,\n",
      "          1,  85,  73,  66,  85,  13,   1,  67,  70,  90,  80,  79,  69,   1,\n",
      "         74,  85,  84,   1,  78,  66,  84,  85,  70,  83,  90,   1,  80,  71,\n",
      "          1,  77,  66,  79,  72,  86,  66,  72,  70,  13,   1,  40,  49,  53,\n",
      "         14,  21,   1,  68,  66,  79,   1,  84,  80,  77,  87,  70,   1,  79,\n",
      "         80,  87,  70,  77,   1,  66,  79,  69,   1,  69,  74,  71,  71,  74,\n",
      "         68,  86,  77,  85,   1,  85,  66,  84,  76,  84,   1,  85,  73,  66,\n",
      "         85,   1,  84,  81,  66,  79,   1,  78,  66,  85,  73,  70,  78,  66,\n",
      "         85,  74,  68,  84,  13,   1,  68,  80,  69,  74,  79,  72,  13,   1,\n",
      "         87,  74,  84,  74,  80,  79,  13,   1,  78,  70,  69,  74,  68,  74,\n",
      "         79,  70,  13,   1,  77,  66,  88,  13,   1,  81,  84,  90,  68,  73,\n",
      "         80,  77,  80,  72,  90,   1,  66,  79,  69,   1,  78,  80,  83,  70,\n",
      "         13,   1,  88,  74,  85,  73,  80,  86,  85,   1,  79,  70,  70,  69,\n",
      "         74,  79,  72,   1,  66,  79,  90,   1,  84,  81,  70,  68,  74,  66,\n",
      "         77,   1,  81,  83,  80,  78,  81,  85,  74,  79,  72,  15,   1,  46,\n",
      "         80,  83,  70,  80,  87,  70,  83,  13,   1,  74,  79,   1,  66,  77,\n",
      "         77,   1,  80,  71,   1,  85,  73,  70,  84,  70,   1,  85,  66,  84,\n",
      "         76,  84,  13,   1,  40,  49,  53,  14,  21, 114,  84,   1,  81,  70,\n",
      "         83,  71,  80,  83,  78,  66])\n"
     ]
    }
   ],
   "source": [
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7246e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11ab3842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(403880, 100970)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b54398a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb:\n",
      "tensor([[ 40,  49,  53,  14,  21, 114,  84,   1],\n",
      "        [ 66,  77,  15,   1,  34,  79,  69,   1],\n",
      "        [  1,  90,   1,  30,   1,  90, 127,   1],\n",
      "        [ 76,   1,  80,  83,   1,  84,  68,  83]])\n",
      "\n",
      "yb:\n",
      "tensor([[ 49,  53,  14,  21, 114,  84,   1,  68],\n",
      "        [ 77,  15,   1,  34,  79,  69,   1,  74],\n",
      "        [ 90,   1,  30,   1,  90, 127,   1,  66],\n",
      "        [  1,  80,  83,   1,  84,  68,  83,  66]])\n",
      "--------------------\n",
      "For context tensor([40]), target is: 49\n",
      "For context tensor([40, 49]), target is: 53\n",
      "For context tensor([40, 49, 53]), target is: 14\n",
      "For context tensor([40, 49, 53, 14]), target is: 21\n",
      "For context tensor([40, 49, 53, 14, 21]), target is: 114\n",
      "For context tensor([ 40,  49,  53,  14,  21, 114]), target is: 84\n",
      "For context tensor([ 40,  49,  53,  14,  21, 114,  84]), target is: 1\n",
      "For context tensor([ 40,  49,  53,  14,  21, 114,  84,   1]), target is: 68\n",
      "---\n",
      "For context tensor([66]), target is: 77\n",
      "For context tensor([66, 77]), target is: 15\n",
      "For context tensor([66, 77, 15]), target is: 1\n",
      "For context tensor([66, 77, 15,  1]), target is: 34\n",
      "For context tensor([66, 77, 15,  1, 34]), target is: 79\n",
      "For context tensor([66, 77, 15,  1, 34, 79]), target is: 69\n",
      "For context tensor([66, 77, 15,  1, 34, 79, 69]), target is: 1\n",
      "For context tensor([66, 77, 15,  1, 34, 79, 69,  1]), target is: 74\n",
      "---\n",
      "For context tensor([1]), target is: 90\n",
      "For context tensor([ 1, 90]), target is: 1\n",
      "For context tensor([ 1, 90,  1]), target is: 30\n",
      "For context tensor([ 1, 90,  1, 30]), target is: 1\n",
      "For context tensor([ 1, 90,  1, 30,  1]), target is: 90\n",
      "For context tensor([ 1, 90,  1, 30,  1, 90]), target is: 127\n",
      "For context tensor([  1,  90,   1,  30,   1,  90, 127]), target is: 1\n",
      "For context tensor([  1,  90,   1,  30,   1,  90, 127,   1]), target is: 66\n",
      "---\n",
      "For context tensor([76]), target is: 1\n",
      "For context tensor([76,  1]), target is: 80\n",
      "For context tensor([76,  1, 80]), target is: 83\n",
      "For context tensor([76,  1, 80, 83]), target is: 1\n",
      "For context tensor([76,  1, 80, 83,  1]), target is: 84\n",
      "For context tensor([76,  1, 80, 83,  1, 84]), target is: 68\n",
      "For context tensor([76,  1, 80, 83,  1, 84, 68]), target is: 83\n",
      "For context tensor([76,  1, 80, 83,  1, 84, 68, 83]), target is: 66\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print('xb:')\n",
    "print(xb)\n",
    "print()\n",
    "print('yb:')\n",
    "print(yb)\n",
    "print('--------------------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        print(f'For context {xb[b,:t+1]}, target is: {yb[b,t]}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0e8ec250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 142])\n",
      "5.392546653747559\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m.forward(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d3f068f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.955827057601261"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# expected loss\n",
    "-math.log(1/vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ae49e855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 77,  80,  69,  90,   1,  85,  73,  66,  48,  33, 138,  24, 101,  47,\n",
       "          22, 130,  27,  12],\n",
       "        [ 53,  14,  21,   1,  66,  83,  70,   1, 136,  92,   2,  57,   1, 108,\n",
       "          45,  58, 119,  55],\n",
       "        [ 73,  70,   1,  82,  86,  70,  84,  85,  68, 117, 115, 106,  19,  87,\n",
       "          12,  89,  28, 118],\n",
       "        [ 69,   1,  58,   1,  66,  83,  70,   1,  47, 135,  44,   5,  85,  79,\n",
       "         108,   1, 135,  86],\n",
       "        [  1,   1,   1,   1,   1,   1,  71,  66, 135,  13,  94, 135,  51,  12,\n",
       "          67,  79,  23,  65],\n",
       "        [ 71,  74,  68,  74,  70,  79,  85,  77,  17,  58,  18,  68,  81,   1,\n",
       "          72,  39, 106, 102],\n",
       "        [ 84,   1,  85,  73,  66,  85,   1,  85,  17,  31,  26, 129, 126, 101,\n",
       "         127, 127,  99,  19],\n",
       "        [  0,  14,   1,  65,  18,  29,  30,  76,  50, 121,  61,  18,  20,   5,\n",
       "         138,  72, 132, 123],\n",
       "        [ 79,  87,  80,  77,  87,  70,  84,   1,  19,  54,  47,  86, 103,  15,\n",
       "          34,   6, 135, 105],\n",
       "        [ 70,  79,   1,  85,  73,  70,   1,  72,  20, 117,  83,  50,  46,  78,\n",
       "           7,  48,  68, 124],\n",
       "        [ 73,  66,  84,   1,  67,  70,  70,  79,  67,   9,  24,  98,  46,  49,\n",
       "          75,  63,  27,  61],\n",
       "        [ 73,  78,  70,  85,  74,  68,   1,  81,  25,  57,   2,  28, 113,  99,\n",
       "          33,  58,  33,  62],\n",
       "        [ 74,  79,   1,  85,  73,  70,   1,  53,  73,  83,  61,  82,   8,  20,\n",
       "          86,  46,  60,  98],\n",
       "        [ 83,  70,   1,  68,  77,  80,  84,  70,  92,  86,   1,  81,  76,  31,\n",
       "         112, 119,  58,  91],\n",
       "        [ 80,  88,  77,  70,  69,  72,  70,   1,  33,  20,  82,  92,   0, 129,\n",
       "           5, 137, 124,  66],\n",
       "        [ 77,  77,  70,  79,  72,  70,  84,   1,  33,  29,  10, 129,  38,  21,\n",
       "         139,  37, 114,  49],\n",
       "        [ 45,  86,  76,  70,   1,  85,  80,  77,  83, 134,  23, 141,  40,  21,\n",
       "          23,  58, 105,  63],\n",
       "        [ 21,  27,   0,   1,   1,   1,   1,  48,  70,   9, 124, 135, 139,  47,\n",
       "         140, 109,  31, 102],\n",
       "        [  1,  83,  86,  79,  15,   1,  56,  70,  27,  41, 106,  82, 110,  10,\n",
       "         129, 111, 101,  64],\n",
       "        [  1,  85,  73,  70,   1,  84,  85,  66,  93, 107, 100,  62,  86,  55,\n",
       "         104,  17, 115, 137],\n",
       "        [ 70,  71,  70,  83,  83,  74,  79,  72, 131,  22,  93, 123,  15,  69,\n",
       "         136, 117,  39, 112],\n",
       "        [ 72,  77,  66,  84,   1,  35,  15,   1, 136,  81,  44,  63, 102,  72,\n",
       "          33,  45,  54,  78],\n",
       "        [ 71,  80,  83,  78,  66,  77,  13,   1, 108,  96,  75,  27,  51, 115,\n",
       "          88, 108, 130,   1],\n",
       "        [  1,  86,  84,  70,   1,  85,  73,  70,  97, 125,  96,  39,  81, 113,\n",
       "           9, 112, 102, 141],\n",
       "        [ 69,   1,  74,  69,  70,  79,  85,  74,  77,  61,  40,  95,  36,  28,\n",
       "          43,  41, 140,  59],\n",
       "        [ 77,  70,  78,   1,  88,  74,  85,  73, 123,  15,  18, 118,  35, 111,\n",
       "           7,  43,  92,   4],\n",
       "        [ 86,  85,  80,  74,  78,   0,  78,  86,   1, 123,  55,  41, 140,   4,\n",
       "           2,  75,  53,  37],\n",
       "        [ 84,   1,  85,  73,  70,   1,  87,  66,   0,  51, 122,  43,  40,  73,\n",
       "         103,   5, 117,  98],\n",
       "        [ 49,  53,  14,  21,   1,  80,  71,  85, 131, 129,  27,  84, 101, 123,\n",
       "         103, 123, 127,  85],\n",
       "        [ 73,  70,  83,   1,  68,  66,  84,  70, 131,  51,  85,  47, 127, 103,\n",
       "          10,  75, 120,  72],\n",
       "        [ 73,  70,  79,   1,  74,  85,   1,  74,  47,  94,  48,  97,  63,  20,\n",
       "         136,  81,  33, 124],\n",
       "        [ 73,  70,  68,  76,  74,  79,  72,   0,  87, 108,  82,  88, 110,  46,\n",
       "          65, 119,  64,  14]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.generate(xb, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "eae7e0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lody tha',\n",
       " 'T-4 are ',\n",
       " 'he quest',\n",
       " 'd Y are ',\n",
       " '      fa',\n",
       " 'ficientl',\n",
       " 's that t',\n",
       " '\\n- `1<=k',\n",
       " 'nvolves ',\n",
       " 'en the g',\n",
       " 'has been',\n",
       " 'hmetic p',\n",
       " 'in the T',\n",
       " 're close',\n",
       " 'owledge ',\n",
       " 'llenges ',\n",
       " 'Luke tol',\n",
       " '4:\\n    O',\n",
       " ' run. We',\n",
       " ' the sta',\n",
       " 'eferring',\n",
       " 'glas B. ',\n",
       " 'formal, ',\n",
       " ' use the',\n",
       " 'd identi',\n",
       " 'lem with',\n",
       " 'utoim\\nmu',\n",
       " 's the va',\n",
       " 'PT-4 oft',\n",
       " 'her case',\n",
       " 'hen it i',\n",
       " 'hecking\\n']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_tensor(x_encoded):\n",
    "    b, t = x_encoded.shape\n",
    "    decoded_strings = []\n",
    "    for i in range(b):\n",
    "        int_list = x_encoded[i, :].tolist()\n",
    "        decoded_string = decode(int_list)\n",
    "        decoded_strings.append(decoded_string)\n",
    "    return decoded_strings\n",
    "\n",
    "decode_tensor(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4103f689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e497874d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nso@vρI“ek\"±ω\\uf8ef—⇒Z@uAQ-U;′l5qh{\"QCnP\\'j\\n9}Sσ⃗∆rcNPV}×\\'\\uf8ee{2g’∪*UZ—L°”yˆEt∞g°×!2}bz|Nbø×∗hD×∗Av⃗N>&øJ7?\\uf8f9@J']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generation\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "decode_tensor(m.generate(idx, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6247de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f00caf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2489973e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3515543937683105\n",
      "3.6309425830841064\n",
      "3.0488831996917725\n",
      "2.8627829551696777\n",
      "2.7648069858551025\n",
      "2.636763095855713\n",
      "2.6641106605529785\n",
      "2.6322357654571533\n",
      "2.6348443031311035\n",
      "2.740755558013916\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for i in range(10):\n",
    "    for steps in range(1000):\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        logits, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "db22b270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ndg ala t thi∗√3 cowalext Liv cathesltstugis QFiora anatotifa whesit M.mingetorat urst?vindend in eri']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generation\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "decode_tensor(m.generate(idx, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d65d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34f2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "fca73024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorization trick for self attention\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# SELF ATTENTION\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**(-0.5)\n",
    "\n",
    "\n",
    "# to jest to samo co średnia od 0 do danego t w każdym przykładzie (out jest średnią)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "21cb586f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n",
       "         [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n",
       "         [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4016, 0.5984, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3365, 0.2271, 0.4364, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3019, 0.2060, 0.2899, 0.2022, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1058, 0.1700, 0.1530, 0.3451, 0.2261, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1526, 0.1645, 0.1357, 0.2684, 0.1919, 0.0869, 0.0000, 0.0000],\n",
       "         [0.1103, 0.1711, 0.0761, 0.1654, 0.1667, 0.1643, 0.1461, 0.0000],\n",
       "         [0.1770, 0.1063, 0.1198, 0.0943, 0.1697, 0.1205, 0.1052, 0.1073]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4955, 0.5045, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2861, 0.3657, 0.3483, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1242, 0.3939, 0.1981, 0.2838, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3531, 0.1668, 0.1768, 0.1813, 0.1220, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1553, 0.1779, 0.1492, 0.1539, 0.1723, 0.1914, 0.0000, 0.0000],\n",
       "         [0.0722, 0.1255, 0.1119, 0.1896, 0.1537, 0.1918, 0.1552, 0.0000],\n",
       "         [0.1344, 0.1368, 0.0970, 0.1395, 0.1292, 0.1304, 0.0790, 0.1535]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5351, 0.4649, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3776, 0.4907, 0.1317, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3079, 0.2849, 0.2206, 0.1865, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2074, 0.2611, 0.1368, 0.2071, 0.1876, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1733, 0.3004, 0.0656, 0.1682, 0.1669, 0.1255, 0.0000, 0.0000],\n",
       "         [0.1216, 0.1213, 0.1416, 0.1119, 0.1439, 0.2213, 0.1383, 0.0000],\n",
       "         [0.0925, 0.1598, 0.0945, 0.1355, 0.1356, 0.1086, 0.1185, 0.1548]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7285463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
